{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1eb8cf489a337c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:04.047466600Z",
     "start_time": "2023-10-30T03:29:02.631791900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GangsuKim\\Anaconda3\\envs\\HLAI2023\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from torchvision.models import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import AutoAugment\n",
    "from torchvision.transforms import AutoAugmentPolicy\n",
    "from torchvision.transforms import transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.imgaug.transforms import IAAPiecewiseAffine\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from CustomLoader import CustomLoader\n",
    "import timm\n",
    "from utils.CosineAnnealingWarmUpRestarts import  CosineAnnealingWarmUpRestarts\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.AugMix import *\n",
    "from utils.CutMix import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b774aba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:04.109004500Z",
     "start_time": "2023-10-30T03:29:04.049926700Z"
    },
    "execution": {
     "iopub.execute_input": "2023-10-23T14:10:00.706380Z",
     "iopub.status.busy": "2023-10-23T14:10:00.705910Z",
     "iopub.status.idle": "2023-10-23T14:10:05.499515Z",
     "shell.execute_reply": "2023-10-23T14:10:05.498153Z"
    },
    "papermill": {
     "duration": 4.809141,
     "end_time": "2023-10-23T14:10:05.503022",
     "exception": false,
     "start_time": "2023-10-23T14:10:00.693881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_set, test_set = getDataSet('./dataset/')\n",
    "set_random_seed(1813)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26369a9422657b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stratic Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afcc638e0e1d66f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:04.124004600Z",
     "start_time": "2023-10-30T03:29:04.112004500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = {}\n",
    "\n",
    "for ts in training_set:\n",
    "    class_n = ts.split('/')[3]\n",
    "\n",
    "    if class_n in classes.keys():\n",
    "        classes[class_n].append(ts)\n",
    "    else:\n",
    "        classes[class_n] = [ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c4f539d0182c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:04.156149100Z",
     "start_time": "2023-10-30T03:29:04.142078600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = getDistributionDataSet(training_set)\n",
    "dist = dict(sorted(dist.items(), key=lambda x:int(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75923c31ee1ceda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:04.200594700Z",
     "start_time": "2023-10-30T03:29:04.156149100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 500,\n",
       " '1': 500,\n",
       " '2': 500,\n",
       " '3': 450,\n",
       " '4': 450,\n",
       " '5': 450,\n",
       " '6': 400,\n",
       " '7': 400,\n",
       " '8': 400,\n",
       " '9': 350,\n",
       " '10': 350,\n",
       " '11': 350,\n",
       " '12': 300,\n",
       " '13': 300,\n",
       " '14': 300,\n",
       " '15': 250,\n",
       " '16': 250,\n",
       " '17': 250,\n",
       " '18': 200,\n",
       " '19': 200,\n",
       " '20': 200,\n",
       " '21': 150,\n",
       " '22': 150,\n",
       " '23': 150,\n",
       " '24': 100,\n",
       " '25': 100,\n",
       " '26': 100,\n",
       " '27': 50,\n",
       " '28': 50,\n",
       " '29': 50}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e3c0373d689c98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:04.205594100Z",
     "start_time": "2023-10-30T03:29:04.171382700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_k = 5\n",
    "\n",
    "fold_dict = {}\n",
    "\n",
    "for i in range(fold_k):\n",
    "    fold_dict[i] = []\n",
    "    \n",
    "for k in classes.keys():\n",
    "    ls = classes[k]\n",
    "    random.shuffle(ls)\n",
    "    ls = np.array(ls)\n",
    "    splited_ls = np.split(ls, fold_k)\n",
    "    \n",
    "    for i, l in enumerate(splited_ls):\n",
    "        fold_dict[i] += list(l)\n",
    "    \n",
    "    \n",
    "training_set = []\n",
    "\n",
    "for i in range(fold_k):\n",
    "    training_set += fold_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624bc32ecfbb2c48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:04.205594100Z",
     "start_time": "2023-10-30T03:29:04.187503700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'AUG_MODE' : 'albu', # or [albu,transform]\n",
    "    'MEAN' : [0.5051, 0.4853, 0.4409],\n",
    "    'STD' : [0.2774, 0.2568, 0.2795],\n",
    "    'CutMix': True,\n",
    "    'mix_prob' : 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699a7dc026f9327",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43c522372ef7611f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:04.217669900Z",
     "start_time": "2023-10-30T03:29:04.203594800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weight = []\n",
    "\n",
    "dist = getDistributionDataSet(training_set)\n",
    "dist = dict(sorted(dist.items(), key=lambda x:int(x[0])))\n",
    "\n",
    "sum_of_dist = 0\n",
    "\n",
    "for k in dist.keys():\n",
    "    sum_of_dist += dist[k]\n",
    "\n",
    "for k in dist.keys():\n",
    "    class_weight.append(1 - (dist[k] / sum_of_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf462c568265843c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d870cd911a0ac63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:04.249902700Z",
     "start_time": "2023-10-30T03:29:04.233771500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTransform(train_mean = [.5, .5, .5], train_std= [.5, .5, .5], val_mean= [.5, .5, .5], val_std= [.5, .5, .5], aug_mode='albu'):\n",
    "    if aug_mode == 'albu':\n",
    "        transform_train = A.Compose([\n",
    "            A.Resize(32,32),\n",
    "            A.Rotate(limit=(-360,360), interpolation=1, border_mode=1),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(train_mean, train_std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "        transform_test = A.Compose([\n",
    "            A.Resize(32,32),\n",
    "            A.Normalize(val_mean, val_std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
    "            transforms.Normalize(train_mean, train_std),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.Normalize(val_mean, val_std),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "    return transform_train, transform_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a92e36bc0a710c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c8b312fe8daa2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:08.533094400Z",
     "start_time": "2023-10-30T03:29:04.250903800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_norm, train_std = getNormStd(training_set)\n",
    "transform_train, transform_valid = getTransform(train_norm, train_std, train_norm, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51b535c8977f1a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:29:08.548094800Z",
     "start_time": "2023-10-30T03:29:08.537094100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8250, 8250)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = []\n",
    "for file in training_set:\n",
    "    all_labels.append(int(file.split('/')[3]))\n",
    "len(all_labels), len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e93c5627a9a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T03:44:10.653305200Z",
     "start_time": "2023-10-30T03:29:08.556094200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "for foldk, (train_idx, val_idx) in enumerate(kf.split(X=training_set, y=all_labels)):\n",
    "    print(f'============= Fold-{foldk} strat =============')\n",
    "    model = timm.create_model(model_name='xception41p', pretrained=False, num_classes=30, drop_rate=0.1)\n",
    "    \n",
    "    lr = 0.000\n",
    "    \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_dataset = CustomLoader(training_set, transforms=transform_train, is_train=True, aug_mode=CFG['AUG_MODE'])\n",
    "    valid_dataset = CustomLoader(training_set, transforms=transform_valid, is_train=True, aug_mode=CFG['AUG_MODE'])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size = 64, num_workers=4, sampler=train_subsampler)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size = 64, num_workers=4, sampler=valid_subsampler)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = torch.tensor(class_weight).to(device))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9,0.999), weight_decay=0.03)\n",
    "    scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0 = 100, T_mult=1, eta_max=0.001, T_up=10, gamma=0.5)\n",
    "    \n",
    "    # Train\n",
    "    n_epochs = 200\n",
    "    EPOCH_FROM = 0\n",
    "    \n",
    "    train_loss = torch.zeros(n_epochs)\n",
    "    valid_loss = torch.zeros(n_epochs)\n",
    "    \n",
    "    train_acc = torch.zeros(n_epochs)\n",
    "    valid_acc = torch.zeros(n_epochs)\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    past_lr = lr\n",
    "    model.to(device)\n",
    "    \n",
    "    last_loss_update = 0\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        # Trian\n",
    "        model.train()\n",
    "        for (image, labels, _) in tqdm(train_loader):\n",
    "            image, labels = image.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if CFG['CutMix'] and e < n_epochs - 10:\n",
    "                mix_decision = np.random.rand()\n",
    "                if mix_decision  < CFG['mix_prob']:\n",
    "                    image, labels = cutmix(image, labels, 1.0)\n",
    "            else:\n",
    "                mix_decision = 1\n",
    "                \n",
    "            pred = model(image)\n",
    "            \n",
    "            if mix_decision < CFG['mix_prob']:\n",
    "                loss = criterion(pred, labels[0]) * labels[2] + criterion(pred, labels[1]) * (1-labels[2])\n",
    "            else:\n",
    "                loss = criterion(pred, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss[e] += loss.item()\n",
    "    \n",
    "            ps = F.softmax(pred, dim=1)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            \n",
    "        \n",
    "        train_loss[e] /= len(train_loader)\n",
    "        train_acc[e] /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for image, labels, _ in tqdm(valid_loader):\n",
    "                image, labels = image.to(device), labels.to(device)\n",
    "                \n",
    "                logits = model(image)\n",
    "                loss = criterion(logits, labels)\n",
    "                valid_loss[e] += loss.item()\n",
    "    \n",
    "                ps = F.softmax(logits, dim=1)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.reshape(top_class.shape)\n",
    "                valid_acc[e] += torch.mean(equals.type(torch.float)).detach().cpu()\n",
    "    \n",
    "        valid_loss[e] /= len(valid_loader)\n",
    "        valid_acc[e] /= len(valid_loader)\n",
    "        \n",
    "        scheduler.step(epoch=e)\n",
    "        \n",
    "        print('Fold: {} \\tEpoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(foldk, e, train_loss[e], valid_loss[e]))\n",
    "        print('Fold: {} \\tEpoch: {} \\tTraining accuracy: {:.6f} \\tValidation accuracy: {:.6f}'.format(foldk, e, train_acc[e], valid_acc[e]))\n",
    "        \n",
    "        print(optimizer.param_groups[-1]['lr'])\n",
    "            \n",
    "        if valid_loss[e] <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss[e]))\n",
    "            torch.save(model.state_dict(), f'./models/replay_test/best_model_fold{foldk}.pth')\n",
    "            valid_loss_min = valid_loss[e]\n",
    "            last_loss_update = e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40.642315,
   "end_time": "2023-10-23T14:10:33.298994",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-23T14:09:52.656679",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
